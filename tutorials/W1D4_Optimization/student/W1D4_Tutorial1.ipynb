{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiiambobby/NMA_Course/blob/main/tutorials/W1D4_Optimization/student/W1D4_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Xa0aYbARtN7C"
      },
      "source": [
        "# Tutorial 1: Optimization techniques\n",
        "\n",
        "**Week 1, Day 5: Optimization**\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Jose Gallego-Posada, Ioannis Mitliagkas\n",
        "\n",
        "__Content reviewers:__ Piyush Chauhan, Vladimir Haltakov, Siwei Bai, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Charles J Edelson, Gagana B, Spiros Chavlis\n",
        "\n",
        "__Production editors:__ Arush Tagade, R. Krishnakumaran, Gagana B, Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gzdtYz3DtN7H"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "Objectives:\n",
        "*   Necessity and importance of optimization\n",
        "*   Introduction to commonly used optimization techniques\n",
        "*   Optimization in non-convex loss landscapes\n",
        "*   'Adaptive' hyperparameter tuning\n",
        "*   Ethical concerns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Gdj-m4BetN7J"
      },
      "outputs": [],
      "source": [
        "# @title Tutorial slides\n",
        "from IPython.display import IFrame\n",
        "link_id = \"ft2sz\"\n",
        "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
        "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "PPiOimi8tN7P"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "UHNMsaTbtN7R",
        "outputId": "4d80f840-fc57-43e7-fefd-859b36330c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for vibecheck (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for datatops (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Install and import feedback gadget\n",
        "\n",
        "!pip3 install vibecheck datatops --quiet\n",
        "\n",
        "from vibecheck import DatatopsContentReviewContainer\n",
        "def content_review(notebook_section: str):\n",
        "    return DatatopsContentReviewContainer(\n",
        "        \"\",  # No text prompt\n",
        "        notebook_section,\n",
        "        {\n",
        "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
        "            \"name\": \"neuromatch_dl\",\n",
        "            \"user_key\": \"f379rz8y\",\n",
        "        },\n",
        "    ).render()\n",
        "\n",
        "\n",
        "feedback_prefix = \"W1D5_T1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "Av4g5_hItN7S"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import copy\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "PjJVudGTtN7U"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "import logging\n",
        "logging.getLogger('matplotlib.font_manager').disabled = True\n",
        "\n",
        "import ipywidgets as widgets  # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")\n",
        "plt.rc('axes', unicode_minus=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "uOVjQH68tN7V"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "def print_params(model):\n",
        "  \"\"\"\n",
        "  Lists the name and current value of the model's\n",
        "  named parameters\n",
        "\n",
        "  Args:\n",
        "    model: an nn.Module inherited model\n",
        "      Represents the ML/DL model\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name, param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "l9l-bD61tN7V"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call the `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Handles variability by controlling sources of randomness\n",
        "  through set seed values\n",
        "\n",
        "  Args:\n",
        "    seed: Integer\n",
        "      Set the seed value to given integer.\n",
        "      If no seed, set seed value to random integer in the range 2^32\n",
        "    seed_torch: Bool\n",
        "      Seeds the random number generator for all devices to\n",
        "      offer some guarantees on reproducibility\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "8jnpoKN3tN7W"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules are used.\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "Oqf7RsSmtN7X",
        "outputId": "6cc1aaf0-42d9-4d3d-88a2-8bd5afb5703b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is enabled in this notebook.\n"
          ]
        }
      ],
      "source": [
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-WvgchJftN7X"
      },
      "source": [
        "---\n",
        "# Section 1. Introduction\n",
        "\n",
        "*Time estimate: ~15 mins*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "se5dkAiRtN7Y",
        "outputId": "d0e5ea3f-5581-411c-b18e-1f246314b417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "00df399a49ed4e04bf4ef78c8a8abfe4",
            "d95c9825b2cf40bd8bfcccfa42813398",
            "149027270be24c2a8c0c2d3f04e9f202",
            "24f1b984b05748ecbe4f259112598faf",
            "c3ad5d57a5384513b8f0c52795635ce8",
            "b91a6732a62b44d8ad79ca05d97be2ea"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00df399a49ed4e04bf4ef78c8a8abfe4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Video 1: Introduction\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'zm9oekdkJbQ'), ('Bilibili', 'BV1VB4y1K7Vr')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "o2PU09J0tN7Z"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Introduction_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EhbpkcNvtN7Z"
      },
      "source": [
        "## Discuss: Unexpected consequences\n",
        "\n",
        "Can you think of examples from your own experience/life where poorly chosen incentives or objectives have led to unexpected consequences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lx-rtuR_tN7a"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_1ecffd5a.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Zw9O6COktN7a"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Unexpected_consequences_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UQle_w72tN7a"
      },
      "source": [
        "---\n",
        "# Section 2: Case study: successfully training an MLP for image classification\n",
        "\n",
        "*Time estimate: ~40 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UcsGWxVbtN7b"
      },
      "source": [
        "Many of the core ideas (and tricks) in modern optimization for deep learning can be illustrated in the simple setting of training an MLP to solve an image classification task. In this tutorial we will guide you through the key challenges that arise when optimizing high-dimensional, non-convex$^\\dagger$ problems. We will use these challenges to motivate and explain some commonly used solutions.\n",
        "\n",
        "**Disclaimer:** Some of the functions you will code in this tutorial are already implemented in Pytorch and many other libraries. For pedagogical reasons, we decided to bring these simple coding tasks into the spotlight and place a relatively higher emphasis in your understanding of the algorithms, rather than the use of a specific library.\n",
        "\n",
        "In 'day-to-day' research projects you will likely rely on the community-vetted, optimized libraries rather than the 'manual implementations' you will write today. In Section 8 you will have a chance to 'put it all together' and use the full power of Pytorch to tune the parameters of an MLP to classify handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dVEmgkPLtN7b"
      },
      "source": [
        "$^\\dagger$: A **strictly convex** function has the same global and local minimum - a nice property for optimization as it won't get stuck in a local minimum that isn't a global one (e.g., $f(x)=x^2 + 2x + 1$). A **non-convex** function is wavy - has some 'valleys' (local minima) that aren't as deep as the overall deepest 'valley' (global minimum). Thus, the optimization algorithms can get stuck in the local minimum, and it can be hard to tell when this happens (e.g., $f(x) = x^4 + x^3 - 2x^2 - 2x$). See also **Section 5** for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "mAQbZuyqtN7d"
      },
      "outputs": [],
      "source": [
        "# @title Video 2: Case Study - MLP Classification\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'pJc2ENhYbqA'), ('Bilibili', 'BV1GB4y1K7Ha')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "SJqghtUttN7e"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Case_study_MLP_classification_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XejGQB7StN7g"
      },
      "source": [
        "## Section 2.1: Data\n",
        "\n",
        "We will use the MNIST dataset of handwritten digits. We load the data via the Pytorch `datasets` module, as you learned in W1D1.\n",
        "\n",
        "**Note:** Although we can download the MNIST dataset directly from `datasets` using the optional argument `download=True`, we are going to download them from NMA directory on OSF to ensure network reliability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "V7iUIKiJtN7g"
      },
      "outputs": [],
      "source": [
        "# @title Download MNIST dataset\n",
        "import tarfile, requests, os\n",
        "\n",
        "fname = 'MNIST.tar.gz'\n",
        "name = 'MNIST'\n",
        "url = 'https://osf.io/y2fj6/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading MNIST dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading MNIST completed.')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall()\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('MNIST dataset has been downloaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "YJv55OSjtN7g"
      },
      "outputs": [],
      "source": [
        "def load_mnist_data(change_tensors=False, download=False):\n",
        "  \"\"\"\n",
        "  Load training and test examples for the MNIST handwritten digits dataset\n",
        "  with every image: 28*28 x 1 channel (greyscale image)\n",
        "\n",
        "  Args:\n",
        "    change_tensors: Bool\n",
        "      Argument to check if tensors need to be normalised\n",
        "    download: Bool\n",
        "      Argument to check if dataset needs to be downloaded/already exists\n",
        "\n",
        "  Returns:\n",
        "    train_set:\n",
        "      train_data: Tensor\n",
        "        training input tensor of size (train_size x 784)\n",
        "      train_target: Tensor\n",
        "        training 0-9 integer label tensor of size (train_size)\n",
        "    test_set:\n",
        "      test_data: Tensor\n",
        "        test input tensor of size (test_size x 784)\n",
        "      test_target: Tensor\n",
        "        training 0-9 integer label tensor of size (test_size)\n",
        "  \"\"\"\n",
        "  # Load train and test sets\n",
        "  train_set = datasets.MNIST(root='.', train=True, download=download,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "  test_set = datasets.MNIST(root='.', train=False, download=download,\n",
        "                            transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "  # Original data is in range [0, 255]. We normalize the data wrt its mean and std_dev.\n",
        "  # Note that we only used *training set* information to compute mean and std\n",
        "  mean = train_set.data.float().mean()\n",
        "  std = train_set.data.float().std()\n",
        "\n",
        "  if change_tensors:\n",
        "    # Apply normalization directly to the tensors containing the dataset\n",
        "    train_set.data = (train_set.data.float() - mean) / std\n",
        "    test_set.data = (test_set.data.float() - mean) / std\n",
        "  else:\n",
        "    tform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                            torchvision.transforms.Normalize(mean=[mean / 255.], std=[std / 255.])\n",
        "                                            ])\n",
        "    train_set = datasets.MNIST(root='.', train=True, download=download,\n",
        "                               transform=tform)\n",
        "    test_set = datasets.MNIST(root='.', train=False, download=download,\n",
        "                              transform=tform)\n",
        "\n",
        "  return train_set, test_set\n",
        "\n",
        "\n",
        "train_set, test_set = load_mnist_data(change_tensors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "2N0od55ItN7g"
      },
      "source": [
        "As we are just getting started, we will concentrate on a small subset of only 500 examples out of the 60.000 data points contained in the whole training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "NsyfJN5vtN7h"
      },
      "outputs": [],
      "source": [
        "# Sample a random subset of 500 indices\n",
        "subset_index = np.random.choice(len(train_set.data), 500)\n",
        "\n",
        "# We will use these symbols to represent the training data and labels, to stay\n",
        "# as close to the mathematical expressions as possible.\n",
        "X, y = train_set.data[subset_index, :], train_set.targets[subset_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "77AsPuxMtN7h"
      },
      "source": [
        "Run the following cell to visualize the content of three examples in our training set. Note how the preprocessing we applied to the data changes the range of pixel values after normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "LbhqX0eetN7h"
      },
      "outputs": [],
      "source": [
        "# @title Run me!\n",
        "\n",
        "# Exploratory data analysis and visualisation\n",
        "\n",
        "num_figures = 3\n",
        "fig, axs = plt.subplots(1, num_figures, figsize=(5 * num_figures, 5))\n",
        "\n",
        "for sample_id, ax in enumerate(axs):\n",
        "  # Plot the pixel values for each image\n",
        "  ax.matshow(X[sample_id, :], cmap='gray_r')\n",
        "  # 'Write' the pixel value in the corresponding location\n",
        "  for (i, j), z in np.ndenumerate(X[sample_id, :]):\n",
        "    text = '{:.1f}'.format(z)\n",
        "    ax.text(j, i, text, ha='center',\n",
        "            va='center', fontsize=6, c='steelblue')\n",
        "\n",
        "  ax.set_title('Label: ' + str(y[sample_id].item()))\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "2anSx9LLtN7h"
      },
      "source": [
        "## Section 2.2: Model\n",
        "\n",
        "As you will see next week, there are specific model architectures that are better suited to image-like data, such as Convolutional Neural Networks (CNNs). For simplicity, in this tutorial we will focus exclusively on Multi-Layer Perceptron (MLP) models as they allow us to highlight many important optimization challenges shared with more advanced neural network designs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Rc6okNCZtN7i"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"\n",
        "  This class implements MLPs in Pytorch of an arbitrary number of hidden\n",
        "  layers of potentially different sizes. Since we concentrate on classification\n",
        "  tasks in this tutorial, we have a log_softmax layer at prediction time.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_dim=784, out_dim=10, hidden_dims=[], use_bias=True):\n",
        "    \"\"\"\n",
        "    Constructs a MultiLayerPerceptron\n",
        "\n",
        "    Args:\n",
        "      in_dim: Integer\n",
        "        dimensionality of input data (784)\n",
        "      out_dim: Integer\n",
        "        number of classes (10)\n",
        "      hidden_dims: List\n",
        "        containing the dimensions of the hidden layers,\n",
        "        empty list corresponds to a linear model (in_dim, out_dim)\n",
        "\n",
        "    Returns:\n",
        "      Nothing\n",
        "    \"\"\"\n",
        "\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "\n",
        "    # If we have no hidden layer, just initialize a linear model (e.g. in logistic regression)\n",
        "    if len(hidden_dims) == 0:\n",
        "      layers = [nn.Linear(in_dim, out_dim, bias=use_bias)]\n",
        "    else:\n",
        "      # 'Actual' MLP with dimensions in_dim - num_hidden_layers*[hidden_dim] - out_dim\n",
        "      layers = [nn.Linear(in_dim, hidden_dims[0], bias=use_bias), nn.ReLU()]\n",
        "\n",
        "      # Loop until before the last layer\n",
        "      for i, hidden_dim in enumerate(hidden_dims[:-1]):\n",
        "        layers += [nn.Linear(hidden_dim, hidden_dims[i + 1], bias=use_bias),\n",
        "                   nn.ReLU()]\n",
        "\n",
        "      # Add final layer to the number of classes\n",
        "      layers += [nn.Linear(hidden_dims[-1], out_dim, bias=use_bias)]\n",
        "\n",
        "    self.main = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Defines the network structure and flow from input to output\n",
        "\n",
        "    Args:\n",
        "      x: Tensor\n",
        "        Image to be processed by the network\n",
        "\n",
        "    Returns:\n",
        "      output: Tensor\n",
        "        same dimension and shape as the input with probabilistic values in the range [0, 1]\n",
        "\n",
        "    \"\"\"\n",
        "    # Flatten each images into a 'vector'\n",
        "    transformed_x = x.view(-1, self.in_dim)\n",
        "    hidden_output = self.main(transformed_x)\n",
        "    output = F.log_softmax(hidden_output, dim=1)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "r1r8sKSrtN7i"
      },
      "source": [
        "Linear models constitute a very special kind of MLPs: they are equivalent to an MLP with *zero* hidden layers. This is simply an affine transformation, in other words a 'linear' map $W x$ with an 'offset' $b$; followed by a softmax function.\n",
        "\n",
        "$$f(x) = \\text{softmax}(W x + b)$$\n",
        "\n",
        "Here $x \\in \\mathbb{R}^{784}$, $W \\in \\mathbb{R}^{10 \\times 784}$ and $b \\in \\mathbb{R}^{10}$. Notice that the dimensions of the weight matrix are $10 \\times 784$ as the input tensors are flattened images, i.e., $28 \\times 28 = 784$-dimensional tensors and the output layer consists of $10$ nodes. Also, note that the implementation of softmax encapsulates b in W i.e., It maps the rows of the input instead of the columns. That is, the i’th row of the output is the mapping of the i’th row of the input under W, plus the bias term. Refer Affine maps here: https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#affine-maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "NRKgBB7LtN7j"
      },
      "outputs": [],
      "source": [
        "# Empty hidden_dims means we take a model with zero hidden layers.\n",
        "model = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
        "\n",
        "# We print the model structure with 784 inputs and 10 outputs\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "NEIovvTrtN7k"
      },
      "source": [
        "## Section 2.3: Loss\n",
        "\n",
        "While we care about the accuracy of the model, the 'discrete' nature of the 0-1 loss makes it challenging to optimize. In order to learn good parameters for this model, we will use the cross entropy loss (negative log-likelihood), which you saw in the last lecture, as a surrogate objective to be minimized.\n",
        "\n",
        "This particular choice of model and optimization objective leads to a *convex* optimization problem with respect to the parameters $W$ and $b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "RvQVWVDltN7k"
      },
      "outputs": [],
      "source": [
        "loss_fn = F.nll_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8wNSzriLtN7l"
      },
      "source": [
        "## Section 2.4: Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kPHGKGCXtN7l"
      },
      "source": [
        "In the last lecture, you saw that inspecting the weights of a model can provide insights on what 'concepts' the model has learned. Here we show the weights of a partially trained model. The weights corresponding to each class 'learn' to _fire_ when an input of the class is detected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "OOo9hYWVtN7l"
      },
      "outputs": [],
      "source": [
        "#@markdown Run _this cell_ to train the model. If you are curious about how the training\n",
        "#@markdown takes place, double-click this cell to find out. At the end of this tutorial\n",
        "#@markdown you will have the opportunity to train a more complex model on your own.\n",
        "\n",
        "cell_verbose = False\n",
        "partial_trained_model = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
        "\n",
        "if cell_verbose:\n",
        "  print('Init loss', loss_fn(partial_trained_model(X), y).item()) # This matches around np.log(10 = # of classes)\n",
        "\n",
        "# Invoke an optimizer using Adaptive gradient and Momentum (more about this in Section 7)\n",
        "optimizer = optim.Adam(partial_trained_model.parameters(), lr=7e-4)\n",
        "for _ in range(200):\n",
        "  loss = loss_fn(partial_trained_model(X), y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "if cell_verbose:\n",
        "  print('End loss', loss_fn(partial_trained_model(X), y).item()) # This should be less than 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "6RR7Wd_4tN7l"
      },
      "outputs": [],
      "source": [
        "# Show class filters of a trained model\n",
        "W = partial_trained_model.main[0].weight.data.numpy()\n",
        "\n",
        "fig, axs = plt.subplots(1, 10, figsize=(15, 4))\n",
        "for class_id in range(10):\n",
        "  axs[class_id].imshow(W[class_id, :].reshape(28, 28), cmap='gray_r')\n",
        "  axs[class_id].axis('off')\n",
        "  axs[class_id].set_title('Class ' + str(class_id) )\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EifCli8VtN7q"
      },
      "source": [
        "---\n",
        "# Section 3: High dimensional search\n",
        "\n",
        "*Time estimate: ~25 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4ZIi1x8_tN7r"
      },
      "source": [
        "We now have a model with its corresponding trainable parameters as well as an objective to optimize. Where do we goto next? How do we find a 'good' configuration of parameters?\n",
        "\n",
        "One idea is to choose a random direction and move only if the objective is reduced. However, this is inefficient in high dimensions and you will see how gradient descent (with a suitable step-size) can guarantee consistent improvement in terms of the objective function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "AxyI1-EitN7r"
      },
      "outputs": [],
      "source": [
        "# @title Video 3: Optimization of an Objective Function\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'aSJTRdjRvvw'), ('Bilibili', 'BV1aL411H7Ce')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "iBCHSKcAtN7r"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Optimization_of_an_Objective_Function_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0rq61ffhtN7s"
      },
      "source": [
        "## Coding Exercise 3: Implement gradient descent\n",
        "\n",
        "In this exercise you will use PyTorch automatic differentiation capabilities to compute the gradient of the loss with respect to the parameters of the model. You will then use these gradients to implement the update performed by the gradient descent method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "yUeDKxmXtN7t"
      },
      "outputs": [],
      "source": [
        "def zero_grad(params):\n",
        "  \"\"\"\n",
        "  Clear gradients as they accumulate on successive backward calls\n",
        "\n",
        "  Args:\n",
        "    params: an iterator over tensors\n",
        "      i.e., updating the Weights and biases\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  for par in params:\n",
        "    if not(par.grad is None):\n",
        "      par.grad.data.zero_()\n",
        "\n",
        "\n",
        "def random_update(model, noise_scale=0.1, normalized=False):\n",
        "  \"\"\"\n",
        "  Performs a random update on the parameters of the model to help\n",
        "  understand the effectiveness of updating random directions\n",
        "  for the problem of optimizing the parameters of a high-dimensional linear model.\n",
        "\n",
        "  Args:\n",
        "    model: nn.Module derived class\n",
        "      The model whose parameters are to be updated\n",
        "\n",
        "    noise_scale: float\n",
        "      Specifies the magnitude of random weight\n",
        "\n",
        "    normalized: Bool\n",
        "      Indicates if the parameter has been normalised or not\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  for par in model.parameters():\n",
        "    noise = torch.randn_like(par)\n",
        "    if normalized:\n",
        "      noise /= torch.norm(noise)\n",
        "    par.data +=  noise_scale * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lAZibGsXtN7x"
      },
      "source": [
        "Let's implement the gradient descent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "vLgbtB2ItN7x"
      },
      "outputs": [],
      "source": [
        "def gradient_update(loss, params, lr=1e-3):\n",
        "  \"\"\"\n",
        "  Perform a gradient descent update on a given loss over a collection of parameters\n",
        "\n",
        "  Args:\n",
        "    loss: Tensor\n",
        "      A scalar tensor containing the loss through which the gradient will be computed\n",
        "    params: List of iterables\n",
        "      Collection of parameters with respect to which we compute gradients\n",
        "    lr: Float\n",
        "      Scalar specifying the learning rate or step-size for the update\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  # Clear up gradients as Pytorch automatically accumulates gradients from\n",
        "  # successive backward calls\n",
        "  zero_grad(params)\n",
        "\n",
        "  # Compute gradients on given objective\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for par in params:\n",
        "      #################################################\n",
        "      ## TODO for students: update the value of the parameter ##\n",
        "      raise NotImplementedError(\"Student exercise: implement gradient update\")\n",
        "      #################################################\n",
        "      # Here we work with the 'data' attribute of the parameter rather than the\n",
        "      # parameter itself.\n",
        "      # Hence - use the learning rate and the parameter's .grad.data attribute to perform an update\n",
        "      par.data -= ...\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "model1 = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
        "print('\\n The model1 parameters before the update are: \\n')\n",
        "print_params(model1)\n",
        "loss = loss_fn(model1(X), y)\n",
        "\n",
        "## Uncomment below to test your function\n",
        "# gradient_update(loss, list(model1.parameters()), lr=1e-1)\n",
        "# print('\\n The model1 parameters after the update are: \\n')\n",
        "# print_params(model1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "NRsGQzPQtN7x"
      },
      "source": [
        "```\n",
        " The model1 parameters after the update are:\n",
        "\n",
        "main.0.weight tensor([[-0.0263,  0.0010,  0.0174,  ...,  0.0298,  0.0278, -0.0220],\n",
        "        [-0.0047, -0.0302, -0.0093,  ..., -0.0077,  0.0248, -0.0240],\n",
        "        [ 0.0234, -0.0237,  0.0335,  ...,  0.0117,  0.0263, -0.0187],\n",
        "        ...,\n",
        "        [-0.0006,  0.0156,  0.0110,  ...,  0.0143, -0.0302, -0.0145],\n",
        "        [ 0.0164,  0.0286,  0.0238,  ..., -0.0127, -0.0191,  0.0188],\n",
        "        [ 0.0206, -0.0354, -0.0184,  ..., -0.0272,  0.0098,  0.0002]])\n",
        "main.0.bias tensor([-0.0292, -0.0018,  0.0115, -0.0370,  0.0054,  0.0155,  0.0317,  0.0246,\n",
        "         0.0198, -0.0061])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jk-vVJastN7y"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_e46fc6a9.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "81M3wcI0tN7y"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Implement_Gradient_descent_Exercise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xwOraHN_tN7y"
      },
      "source": [
        "## Comparing updates\n",
        "\n",
        "These plots compare the effectiveness of updating random directions for the problem of optimizing the parameters of a high-dimensional linear model. We contrast the behavior at initialization and during an intermediate stage of training by showing the histograms of change in loss over 100 different random directions vs the change in loss induced by the gradient descent update\n",
        "\n",
        "**Remember:** Since we are trying to minimize here, the more negative the better!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "xVgvfuBktN7y"
      },
      "outputs": [],
      "source": [
        "# @markdown _Run this cell_ to visualize the results\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "for id, (model_name, my_model) in enumerate([('Initialization', model),\n",
        "                                              ('Partially trained', partial_trained_model)]):\n",
        "  # Compute the loss we will be comparing to\n",
        "  base_loss = loss_fn(my_model(X), y)\n",
        "\n",
        "  # Compute the improvement via gradient descent\n",
        "  dummy_model = copy.deepcopy(my_model)\n",
        "  loss1 = loss_fn(dummy_model(X), y)\n",
        "  gradient_update(loss1, list(dummy_model.parameters()), lr=1e-2)\n",
        "  gd_delta = loss_fn(dummy_model(X), y) - base_loss\n",
        "\n",
        "  deltas = []\n",
        "  for trial_id in range(100):\n",
        "    # Compute the improvement obtained with a random direction\n",
        "    dummy_model = copy.deepcopy(my_model)\n",
        "    random_update(dummy_model, noise_scale=1e-2)\n",
        "    deltas.append((loss_fn(dummy_model(X), y) - base_loss).item())\n",
        "\n",
        "  # Plot histogram for random direction and vertical line for gradient descent\n",
        "  axs[id].hist(deltas, label='Random Directions', bins=20)\n",
        "  axs[id].set_title(model_name)\n",
        "  axs[id].set_xlabel('Change in loss')\n",
        "  axs[id].set_ylabel('% samples')\n",
        "  axs[id].axvline(0, c='green', alpha=0.5)\n",
        "  axs[id].axvline(gd_delta.item(), linestyle='--', c='red', alpha=1,\n",
        "                  label='Gradient Descent')\n",
        "\n",
        "\n",
        "handles, labels = axs[id].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='upper center',\n",
        "           bbox_to_anchor=(0.5, 1.05),\n",
        "           fancybox=False, shadow=False, ncol=2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "FKeAWBkztN7z"
      },
      "source": [
        "## Think! 3: Gradient descent vs. random search\n",
        "\n",
        "Compare the behavior of gradient descent and random search based on the histograms above. Is any of the two methods more reliable? How can you explain the changes between behavior of the methods at initialization vs during training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bdDPLxDXtN70"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_c2013acf.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "HPmy6RdRtN70"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Gradient_descent_vs_random_search_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "FgLwlknotN71"
      },
      "source": [
        "---\n",
        "# Section 4: Poor conditioning\n",
        "\n",
        "*Time estimate: ~30 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "HWnii2IJtN71"
      },
      "source": [
        "Already in this 'simple' logistic regression problem, the issue of bad conditioning is haunting us. Not all parameters are created equal and the sensitivity of the network to changes on the parameters will have a big impact in the dynamics of the optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "PA_n70uFtN71"
      },
      "outputs": [],
      "source": [
        "# @title Video 4: Momentum\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', '3ES5O58Y_2M'), ('Bilibili', 'BV1NL411H71t')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "-eAMJXGvtN72"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Momentum_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6_et2zBhtN73"
      },
      "source": [
        "We illustrate this issue in a 2-dimensional setting. We freeze all but two parameters of the network: one of them is an element of the weight matrix (filter) for class 0, while the other is the bias for class 7. These results in an optimization with two decision variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xPQ2bHAWtN73"
      },
      "source": [
        "### Think 4!: How momentum works?\n",
        "\n",
        "How much difference is there in the behavior of these two parameters under gradient descent? What is the effect of momentum in bridging that gap?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "S2TdDDc5tN73"
      },
      "outputs": [],
      "source": [
        "# to remove solution\n",
        "\"\"\"\n",
        "The landscapes of the two parameters appear to be\n",
        "flatter under gradient descent as can be seen in interactive demo 4 below.\n",
        "\n",
        "As randomly-initialised models exhibit chaos, we use the Newton's approach\n",
        "by tweaking the learning rate i.e., taking smaller steps in the indicated\n",
        "direction and recomputing gradients to find an optimal solution on a\n",
        "varied surface. Momentum helps reduce the chaos by maintaining a consistent\n",
        "direction for exploration (linear combination of the previous heading vector,\n",
        "and the newly-computed gradient vector).\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "UwNW27s5tN73"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_How_Momentum_works_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "1NNLbULRtN74"
      },
      "source": [
        "## Coding Exercise 4: Implement momentum\n",
        "\n",
        "In this exercise you will implement the momentum update given by:\n",
        "\n",
        "\\begin{equation}\n",
        "w_{t+1} = w_t - \\eta \\nabla J(w_t) + \\beta (w_t - w_{t-1})\n",
        "\\end{equation}\n",
        "\n",
        "It is convenient to re-express this update rule in terms of a recursion. For that, we define 'velocity' as the quantity:\n",
        "\\begin{equation}\n",
        "v_{t-1} := w_{t} - w_{t-1}\n",
        "\\end{equation}\n",
        "\n",
        "which leads to the two-step update rule:\n",
        "\n",
        "\\begin{equation}\n",
        "v_t = - \\eta \\nabla J(w_t) + \\beta (\\underbrace{w_t - w_{t-1}}_{v_{t-1}})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "w_{t+1} \\leftarrow w_t + v_{t}\n",
        "\\end{equation}\n",
        "\n",
        "Pay attention to the positive sign of the update in the last equation, given the definition of $v_t$, above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "jMSGEsnNtN74"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to setup some helper functions!\n",
        "\n",
        "def loss_2d(model, u, v, mask_idx=(0, 378), bias_id=7):\n",
        "  \"\"\"\n",
        "  Defines a 2-dim function by freezing all\n",
        "  but two parameters of a linear model.\n",
        "\n",
        "  Args:\n",
        "    model: nn.Module\n",
        "      a pytorch linear model\n",
        "    u: Scalar\n",
        "      first free parameter\n",
        "    u: Scalar\n",
        "      second free parameter\n",
        "    mask_idx: Tuple\n",
        "      selects parameter in weight matrix replaced by u\n",
        "    bias_idx: Integer\n",
        "      selects parameter in bias vector replaced by v\n",
        "\n",
        "  Returns:\n",
        "    loss: Scalar\n",
        "      loss of the 'new' model\n",
        "      over inputs X, y (defined externally)\n",
        "  \"\"\"\n",
        "\n",
        "  # We zero out the element of the weight tensor that will be\n",
        "  # replaced by u\n",
        "  mask = torch.ones_like(model.main[0].weight)\n",
        "  mask[mask_idx[0], mask_idx[1]] = 0.\n",
        "  masked_weights = model.main[0].weight * mask\n",
        "\n",
        "  # u is replacing an element of the weight matrix\n",
        "  masked_weights[mask_idx[0], mask_idx[1]] = u\n",
        "\n",
        "  res = X.reshape(-1, 784) @ masked_weights.T + model.main[0].bias\n",
        "\n",
        "  # v is replacing a bias for class 7\n",
        "  res[:, 7] += v - model.main[0].bias[7]\n",
        "  res =  F.log_softmax(res, dim=1)\n",
        "\n",
        "  return loss_fn(res, y)\n",
        "\n",
        "\n",
        "def plot_surface(U, V, Z, fig):\n",
        "  \"\"\"\n",
        "  Plot a 3D loss surface given\n",
        "  meshed inputs U, V and values Z\n",
        "\n",
        "  Args:\n",
        "    U: nd.array()\n",
        "      Input to plot for obtaining 3D loss surface\n",
        "    V: nd.array()\n",
        "      Input to plot for obtaining 3D loss surface\n",
        "    Z: nd.array()\n",
        "      Input to plot for obtaining 3D loss surface\n",
        "    fig: matplotlib.figure.Figure instance\n",
        "      Helps create a new figure, or activate an existing figure.\n",
        "\n",
        "  Returns:\n",
        "    ax: matplotlib.axes._subplots.AxesSubplot instance\n",
        "      Plotted subplot data\n",
        "  \"\"\"\n",
        "  ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "  ax.view_init(45, -130)\n",
        "\n",
        "  surf = ax.plot_surface(U, V, Z, cmap=plt.cm.coolwarm,\n",
        "                      linewidth=0, antialiased=True, alpha=0.5)\n",
        "\n",
        "  # Select certain level contours to plot\n",
        "  # levels = Z.min() * np.array([1.005, 1.1, 1.3, 1.5, 2.])\n",
        "  # plt.contour(U, V, Z)# levels=levels, alpha=0.5)\n",
        "\n",
        "  ax.set_xlabel('Weight')\n",
        "  ax.set_ylabel('Bias')\n",
        "  ax.set_zlabel('Loss', rotation=90)\n",
        "\n",
        "  return ax\n",
        "\n",
        "\n",
        "def plot_param_distance(best_u, best_v, trajs, fig, styles, labels,\n",
        "                        use_log=False, y_min_v=-12.0, y_max_v=1.5):\n",
        "  \"\"\"\n",
        "  Plot the distance to each of the\n",
        "  two parameters for a collection of 'trajectories'\n",
        "\n",
        "  Args:\n",
        "    best_u: float\n",
        "      Optimal distance of vector u within trajectory\n",
        "    best_v: float\n",
        "      Optimal distance of vector v within trajectory\n",
        "    trajs: Tensor\n",
        "      Specifies trajectories\n",
        "    fig: matplotlib.figure.Figure instance\n",
        "      Helps create a new figure, or activate an existing figure.\n",
        "    styles: Tensor\n",
        "      Specifying Style requirements\n",
        "    use_log: Bool\n",
        "      Specifies if log distance should be calculated; else, absolute distance\n",
        "    y_min_v: float\n",
        "      Minimum distance from y to v\n",
        "    y_max_v: float\n",
        "      Maximum distance from y to v\n",
        "\n",
        "  Returns:\n",
        "    ax: matplotlib.axes._subplots.AxesSubplot instance\n",
        "      Plotted subplot data\n",
        "  \"\"\"\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  for traj, style, label in zip(trajs, styles, labels):\n",
        "    d0 = np.array([np.abs(_[0] - best_u) for _ in traj])\n",
        "    d1 = np.array([np.abs(_[1] - best_v) for _ in traj])\n",
        "    if use_log:\n",
        "      d0 = np.log(1e-16 + d0)\n",
        "      d1 = np.log(1e-16 + d1)\n",
        "    ax.plot(range(len(traj)), d0, style, label='weight - ' + label)\n",
        "    ax.plot(range(len(traj)), d1, style, label='bias - ' + label)\n",
        "  ax.set_xlabel('Iteration')\n",
        "  if use_log:\n",
        "    ax.set_ylabel('Log distance to optimum (per dimension)')\n",
        "    ax.set_ylim(y_min_v, y_max_v)\n",
        "  else:\n",
        "    ax.set_ylabel('Abs distance to optimum (per dimension)')\n",
        "  ax.legend(loc='right', bbox_to_anchor=(1.5, 0.5),\n",
        "            fancybox=False, shadow=False, ncol=1)\n",
        "\n",
        "  return ax\n",
        "\n",
        "\n",
        "def run_optimizer(inits, eval_fn, update_fn, max_steps=500,\n",
        "                  optim_kwargs={'lr':1e-2}, log_traj=True):\n",
        "  \"\"\"\n",
        "  Runs an optimizer on a given\n",
        "  objective and logs parameter trajectory\n",
        "\n",
        "  Args:\n",
        "      inits list: Scalar\n",
        "        initialization of parameters\n",
        "      eval_fn: Callable\n",
        "        function computing the objective to be minimized\n",
        "      update_fn: Callable\n",
        "        function executing parameter update\n",
        "      max_steps: Integer\n",
        "        number of iterations to run\n",
        "      optim_kwargs: Dictionary\n",
        "        customizable dictionary containing appropriate hyperparameters for the chosen optimizer\n",
        "      log_traj: Bool\n",
        "        Specifies if log distance should be calculated; else, absolute distance\n",
        "\n",
        "  Returns:\n",
        "      list: List\n",
        "        trajectory information [*params, loss] for each optimization step\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize parameters and optimizer\n",
        "  params = [nn.Parameter(torch.tensor(_)) for _ in inits]\n",
        "  # Methods like momentum and rmsprop keep and auxiliary vector of parameters\n",
        "  aux_tensors = [torch.zeros_like(_) for _ in params]\n",
        "  if log_traj:\n",
        "    traj = np.zeros((max_steps, len(params)+1))\n",
        "  for _ in range(max_steps):\n",
        "    # Evaluate loss\n",
        "    loss = eval_fn(*params)\n",
        "    # Store 'trajectory' information\n",
        "    if log_traj:\n",
        "      traj[_, :] = [_.item() for _ in params] + [loss.item()]\n",
        "    # Perform update\n",
        "    if update_fn == gradient_update:\n",
        "      gradient_update(loss, params, **optim_kwargs)\n",
        "    else:\n",
        "      update_fn(loss, params, aux_tensors, **optim_kwargs)\n",
        "  if log_traj:\n",
        "    return traj\n",
        "\n",
        "\n",
        "L = 4.\n",
        "xs = np.linspace(-L, L, 30)\n",
        "ys = np.linspace(-L, L, 30)\n",
        "U, V = np.meshgrid(xs, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "agB78QWbtN75"
      },
      "outputs": [],
      "source": [
        "def momentum_update(loss, params, grad_vel, lr=1e-3, beta=0.8):\n",
        "  \"\"\"\n",
        "  Perform a momentum update over a collection of parameters given a loss and velocities\n",
        "\n",
        "  Args:\n",
        "    loss: Tensor\n",
        "      A scalar tensor containing the loss through which gradient will be computed\n",
        "    params: Iterable\n",
        "      Collection of parameters with respect to which we compute gradients\n",
        "    grad_vel: Iterable\n",
        "      Collection containing the 'velocity' v_t for each parameter\n",
        "    lr: Float\n",
        "      Scalar specifying the learning rate or step-size for the update\n",
        "    beta: Float\n",
        "      Scalar 'momentum' parameter\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  # Clear up gradients as Pytorch automatically accumulates gradients from\n",
        "  # successive backward calls\n",
        "  zero_grad(params)\n",
        "  # Compute gradients on given objective\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (par, vel) in zip(params, grad_vel):\n",
        "      #################################################\n",
        "      ## TODO for students: update the value of the parameter ##\n",
        "      raise NotImplementedError(\"Student exercise: implement momentum update\")\n",
        "      #################################################\n",
        "      # Update 'velocity'\n",
        "      vel.data = ...\n",
        "      # Update parameters\n",
        "      par.data += ...\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "model2 = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
        "print('\\n The model2 parameters before the update are: \\n')\n",
        "print_params(model2)\n",
        "loss = loss_fn(model2(X), y)\n",
        "initial_vel = [torch.randn_like(p) for p in model2.parameters()]\n",
        "\n",
        "## Uncomment below to test your function\n",
        "# momentum_update(loss, list(model2.parameters()), grad_vel=initial_vel, lr=1e-1, beta=0.9)\n",
        "# print('\\n The model2 parameters after the update are: \\n')\n",
        "# print_params(model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "HA7Vgb3_tN75"
      },
      "source": [
        "```\n",
        " The model2 parameters after the update are:\n",
        "\n",
        "main.0.weight tensor([[ 1.5898,  0.0116, -2.0239,  ..., -1.0871,  0.4030, -0.9577],\n",
        "        [ 0.4653,  0.6022, -0.7363,  ...,  0.5485, -0.2747, -0.6539],\n",
        "        [-1.4117, -1.1045,  0.6492,  ..., -1.0201,  0.6503,  0.1310],\n",
        "        ...,\n",
        "        [-0.5098,  0.5075, -0.0718,  ...,  1.1192,  0.2900, -0.9657],\n",
        "        [-0.4405, -0.1174,  0.7542,  ...,  0.0792, -0.1857,  0.3537],\n",
        "        [-1.0824,  1.0080, -0.4254,  ..., -0.3760, -1.7491,  0.6025]])\n",
        "main.0.bias tensor([ 0.4147, -1.0440,  0.8720, -1.6201, -0.9632,  0.9430, -0.5180,  1.3417,\n",
        "         0.6574,  0.3677])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "c5Eyx06ttN75"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_3fe0e5cf.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "OVlDSuIJtN76"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Implement_momentum_Exercise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cmugE0HBtN76"
      },
      "source": [
        "## Interactive Demo 4: Momentum vs. GD\n",
        "\n",
        "The plots below show the distance to the optimum for both variables across the two methods, as well as the parameter trajectory over the loss surface.\n",
        "\n",
        "Tune the learning rate and momentum parameters to achieve a loss below $10^{-6}$ (for both dimensions) within 100 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "y40u-0M2tN76"
      },
      "outputs": [],
      "source": [
        "# @markdown Run this cell to enable the widget!\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def run_newton(func, init_list=[0., 0.], max_iter=200):\n",
        "  \"\"\"\n",
        "  Find the optimum of this 2D problem using Newton's method\n",
        "\n",
        "  Args:\n",
        "    func: Callable\n",
        "      Initialising parameter tensor updates\n",
        "    init_list: Scalar\n",
        "      initialization of parameters\n",
        "    max_iter: Integer\n",
        "      The maximum number of iterations to complete\n",
        "\n",
        "  Returns:\n",
        "    par_tensor.data.numpy(): ndarray\n",
        "      List of newton's updates\n",
        "  \"\"\"\n",
        "\n",
        "  par_tensor = torch.tensor(init_list, requires_grad=True)\n",
        "  t_g = lambda par_tensor: func(par_tensor[0], par_tensor[1])\n",
        "\n",
        "  for _ in tqdm(range(max_iter)):\n",
        "    eval_loss = t_g(par_tensor)\n",
        "    eval_grad = torch.autograd.grad(eval_loss, [par_tensor])[0]\n",
        "    eval_hess = torch.autograd.functional.hessian(t_g, par_tensor)\n",
        "    # Newton's update is:  - inverse(Hessian) x gradient\n",
        "    par_tensor.data -= torch.inverse(eval_hess) @ eval_grad\n",
        "\n",
        "  return par_tensor.data.numpy()\n",
        "\n",
        "\n",
        "set_seed(2021)\n",
        "model = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
        "# Define 2d loss objectives and surface values\n",
        "g = lambda u, v: loss_2d(copy.deepcopy(model), u, v)\n",
        "Z = np.fromiter(map(g, U.ravel(), V.ravel()), U.dtype).reshape(V.shape)\n",
        "\n",
        "best_u, best_v  = run_newton(func=g)\n",
        "\n",
        "# Initialization of the variables\n",
        "INITS = [2.5, 3.7]\n",
        "\n",
        "# Used for plotting\n",
        "LABELS = ['GD', 'Momentum']\n",
        "COLORS = ['black', 'red']\n",
        "LSTYLES = ['-', '--']\n",
        "\n",
        "\n",
        "@widgets.interact_manual\n",
        "def momentum_experiment(max_steps=widgets.IntSlider(300, 50, 500, 5),\n",
        "                        lr=widgets.FloatLogSlider(value=1e-1, min=-3, max=0.7, step=0.1),\n",
        "                        beta=widgets.FloatSlider(value=9e-1, min=0, max=1., step=0.01)\n",
        "                        ):\n",
        "  \"\"\"\n",
        "  Displays the momentum experiment as a widget\n",
        "\n",
        "  Args:\n",
        "    max_steps: widget integer slider\n",
        "      Maximum number of steps on the slider with default = 300\n",
        "    lr: widget float slider\n",
        "      Scalar specifying the learning rate or step-size for the update with default = 1e-1\n",
        "    beta: widget float slider\n",
        "      Scalar 'momentum' parameter with default = 9e-1\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  # Execute both optimizers\n",
        "  sgd_traj = run_optimizer(INITS, eval_fn=g, update_fn=gradient_update,\n",
        "                           max_steps=max_steps, optim_kwargs={'lr': lr})\n",
        "  mom_traj = run_optimizer(INITS, eval_fn=g, update_fn=momentum_update,\n",
        "                           max_steps=max_steps, optim_kwargs={'lr': lr, 'beta':beta})\n",
        "\n",
        "  TRAJS = [sgd_traj, mom_traj]\n",
        "\n",
        "  # Plot distances\n",
        "  fig = plt.figure(figsize=(9,4))\n",
        "  plot_param_distance(best_u, best_v, TRAJS, fig,\n",
        "                      LSTYLES, LABELS, use_log=True, y_min_v=-12.0, y_max_v=1.5)\n",
        "\n",
        "  # # Plot trajectories\n",
        "  fig = plt.figure(figsize=(12, 5))\n",
        "  ax = plot_surface(U, V, Z, fig)\n",
        "  for traj, c, label in zip(TRAJS, COLORS, LABELS):\n",
        "    ax.plot3D(*traj.T, c, linewidth=0.3, label=label)\n",
        "    ax.scatter3D(*traj.T, '.-', s=1, c=c)\n",
        "\n",
        "  # Plot optimum point\n",
        "  ax.scatter(best_u, best_v, Z.min(), marker='*', s=80, c='lime', label='Opt.');\n",
        "  lines = [Line2D([0], [0],\n",
        "                  color=c,\n",
        "                  linewidth=3,\n",
        "                  linestyle='--') for c in COLORS]\n",
        "  lines.append(Line2D([0], [0], color='lime', linewidth=0, marker='*'))\n",
        "  ax.legend(lines, LABELS + ['Optimum'], loc='right',\n",
        "            bbox_to_anchor=(.8, -0.1), ncol=len(LABELS) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "bn8jnNeltN77"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Momentum_vs_GD_Interactive_Demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "F5n78AcZtN77"
      },
      "source": [
        "## Think! 4: Momentum and oscillations\n",
        "\n",
        "- Discuss how this specific example illustrates the issue of poor conditioning in optimization? How does momentum help resolve these difficulties?\n",
        "\n",
        "- Do you see oscillations for any of these methods? Why does this happen?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8WahDSCJtN77"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_5eaa9306.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "1uUoTta2tN78"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Momentum_and_oscillations_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "eodCddB3tN7_"
      },
      "source": [
        "---\n",
        "# Section 5: Non-convexity\n",
        "\n",
        "*Time estimate: ~30 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pAdEZTwStN8A"
      },
      "source": [
        "The introduction of even just 1 hidden layer in the neural network transforms the previous convex optimization problem into a non-convex one. And with great non-convexity, comes great responsibility... (Sorry, we couldn't help it!)\n",
        "\n",
        "**Note:** From this section onwards we will be dealing with non-convex optimization problems for the remainder of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5l_xK__vtN8C"
      },
      "outputs": [],
      "source": [
        "# @title Video 5: Overparameterization\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', '7vUpUEKKl5o'), ('Bilibili', 'BV16h41167Jr')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "52ybx_1ftN8C"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Overparameterization_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "KS9inK4atN8C"
      },
      "source": [
        "Take a couple of minutes to play with a more complex 3D visualization of the loss landscape of a neural network on a non-convex problem. Visit https://losslandscape.com/explorer.\n",
        "\n",
        "1. Explore the features on the bottom left corner. You can see an explanation for each icon by clicking on the ( i ) button located on the top right corner.\n",
        "2. Use the 'gradient descent' feature to perform a thought experiment:\n",
        "    -   Choose an initialization\n",
        "    -   Choose the learning rate\n",
        "    -   Mentally formulate your hypothesis about what kind of trajectory you expect to observe\n",
        "3. Run the experiment and contrast your intuition with the observed behavior.\n",
        "4. Repeat this experiment a handful of times for several initialization/learning rate configurations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-l4t1dhptN8D"
      },
      "source": [
        "## Interactive Demo 5: Overparameterization to the rescue!\n",
        "\n",
        "As you may have seen, the non-convex nature of the surface can lead the optimization process to get stuck in undesirable local-optima. There is ample empirical evidence supporting the claim that 'overparameterized' models are easier to train.\n",
        "\n",
        "We will explore this assertion in the context of our MLP training. For this, we initialize a fixed model and construct several models by small random perturbations to the original initialized weights. Now, we train each of these perturbed models and see how the loss evolves. If we were in the convex setting, we should reach very similar objective values upon convergence since all these models were very close at the beginning of training, and in convex problems, the local optimum is also the global optimum.\n",
        "\n",
        "Use the interactive plot below to visualize the loss progression for these perturbed models:\n",
        "\n",
        "1. Select different settings from the `hidden_dims` drop-down menu.\n",
        "2. Explore the effect of the number of steps and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "lU7hL80btN8D"
      },
      "outputs": [],
      "source": [
        "# @markdown Execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact_manual\n",
        "def overparam(max_steps=widgets.IntSlider(150, 50, 500, 5),\n",
        "              hidden_dims=widgets.Dropdown(options=[\"10\", \"20, 20\", \"100, 100\"],\n",
        "                                           value=\"10\"),\n",
        "              lr=widgets.FloatLogSlider(value=5e-2, min=-3, max=0, step=0.1),\n",
        "              num_inits=widgets.IntSlider(7, 5, 10, 1)):\n",
        "  \"\"\"\n",
        "  Displays the overparameterization phenomenon as a widget\n",
        "\n",
        "  Args:\n",
        "    max_steps: widget integer slider\n",
        "      Maximum number of steps on the slider with default = 150\n",
        "    hidden_dims: widget dropdown menu instance\n",
        "      The number of hidden dimensions with default = 10\n",
        "    lr: widget float slider\n",
        "      Scalar specifying the learning rate or step-size for the update with default = 5e-2\n",
        "    num_inits: widget integer slider\n",
        "      Scalar number of epochs\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "\n",
        "  X, y = train_set.data[subset_index, :], train_set.targets[subset_index]\n",
        "\n",
        "  hdims = [int(s) for s in hidden_dims.split(',')]\n",
        "  base_model = MLP(in_dim=784, out_dim=10, hidden_dims=hdims)\n",
        "\n",
        "  fig, axs = plt.subplots(1, 1, figsize=(5, 4))\n",
        "\n",
        "  for _ in tqdm(range(num_inits)):\n",
        "    model = copy.deepcopy(base_model)\n",
        "    random_update(model, noise_scale=2e-1)\n",
        "    loss_hist = np.zeros((max_steps, 2))\n",
        "    for step in range(max_steps):\n",
        "      loss = loss_fn(model(X), y)\n",
        "      gradient_update(loss, list(model.parameters()), lr=lr)\n",
        "      loss_hist[step] = np.array([step, loss.item()])\n",
        "\n",
        "    plt.plot(loss_hist[:, 0], loss_hist[:, 1])\n",
        "\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.ylim(0, 3)\n",
        "  plt.show()\n",
        "\n",
        "  num_params = sum([np.prod(_.shape) for _ in model.parameters()])\n",
        "  print('Number of parameters in model:  ' + str(num_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "j1nf1rhotN8D"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Overparameterization_Interactive_Demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-HH-faEltN8E"
      },
      "source": [
        "### Think! 5.1: Width and depth of the network\n",
        "\n",
        "- We see that as we increase the width/depth of the network, training becomes faster and more consistent across different initializations. What might be the reasons for this behavior?\n",
        "\n",
        "- What are some potential downsides of this approach to dealing with non-convexity?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "2rRWTAJJtN8E"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_d69ca8d7.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "U8HpIR1UtN8E"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Width_and_depth_of_the_network_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lUm1hmoztN8E"
      },
      "source": [
        "---\n",
        "# Section 6: Full gradients are expensive\n",
        "\n",
        "*Time estimate: ~25 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "D748LTxUtN8E"
      },
      "source": [
        "So far we have used only a small (fixed) subset of 500 training examples to perform the updates on the model parameters in our quest to minimize the loss. But what if we decided to use the training set? Do our current approach scale to datasets with tens of thousands, or millions of datapoints?\n",
        "\n",
        "In this section we explore an efficient alternative to avoid having to perform computations on all the training examples before performing a parameter update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "t7xlBY0ntN8F"
      },
      "outputs": [],
      "source": [
        "# @title Video 6: Mini-batches\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'hbqUxpNBUGk'), ('Bilibili', 'BV1ty4y1T7Uh')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "FsryQYSBtN8F"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Mini_batches_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "z3e3yR4JtN8F"
      },
      "source": [
        "## Interactive Demo 6.1: Cost of computation\n",
        "\n",
        "Evaluating a neural network is a relatively fast process. However, when repeated millions of times, the computational cost of performing forward and backward passes through the network starts to become significant.\n",
        "\n",
        "In the visualization below, we show the time (averaged over 5 runs) of computing a forward and backward pass with a changing number of input examples. Choose from the different options in the drop-down box and note how the vertical scale changes depending on the size of the network.\n",
        "\n",
        "**Remarks:** Note that the computational cost of a forward pass shows a clear linear relationship with the number of input examples, and the cost of the corresponding backward pass exhibits a similar computational complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "QatMegMStN8G"
      },
      "outputs": [],
      "source": [
        "# @markdown Execute this cell to enable the widget!\n",
        "\n",
        "def gradient_update(loss, params, lr=1e-3):\n",
        "  \"\"\"\n",
        "  Perform a gradient descent update on a given loss over a collection of parameters\n",
        "\n",
        "  Args:\n",
        "    loss: Tensor\n",
        "      A scalar tensor containing the loss through which the gradient will be computed\n",
        "    params: List of iterables\n",
        "      Collection of parameters with respect to which we compute gradients\n",
        "    lr: Float\n",
        "      Scalar specifying the learning rate or step-size for the update\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  # Clear up gradients as Pytorch automatically accumulates gradients from\n",
        "  # successive backward calls\n",
        "  zero_grad(params)\n",
        "\n",
        "  # Compute gradients on given objective\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for par in params:\n",
        "       par.data -= lr * par.grad.data\n",
        "\n",
        "\n",
        "def measure_update_time(model, num_points):\n",
        "  \"\"\"\n",
        "  Measuring the time for update\n",
        "\n",
        "  Args:\n",
        "    model: an nn.Module inherited model\n",
        "      Represents the ML/DL model\n",
        "    num_points: integer\n",
        "      The number of data points in the train_set\n",
        "\n",
        "  Returns:\n",
        "    tuple of loss time and time for calculation of gradient\n",
        "  \"\"\"\n",
        "  X, y = train_set.data[:num_points], train_set.targets[:num_points]\n",
        "  start_time = time.time()\n",
        "  loss = loss_fn(model(X), y)\n",
        "  loss_time = time.time()\n",
        "  gradient_update(loss, list(model.parameters()), lr=0)\n",
        "  gradient_time = time.time()\n",
        "  return loss_time - start_time, gradient_time - loss_time\n",
        "\n",
        "\n",
        "@widgets.interact\n",
        "def computation_time(hidden_dims=widgets.Dropdown(options=[\"1\", \"100\", \"50, 50\"],\n",
        "                                                  value=\"100\")):\n",
        "  \"\"\"\n",
        "  Demonstrating time taken for computation as a widget\n",
        "\n",
        "  Args:\n",
        "    hidden_dims: widgets dropdown\n",
        "      The number of hidden dimensions with default = 100\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  hdims = [int(s) for s in hidden_dims.split(',')]\n",
        "  model = MLP(in_dim=784, out_dim=10, hidden_dims=hdims)\n",
        "\n",
        "  NUM_POINTS = [1, 5, 10, 100, 200, 500, 1000, 5000, 10000, 20000, 30000, 50000]\n",
        "  times_list = []\n",
        "  for _ in range(5):\n",
        "    times_list.append(np.array([measure_update_time(model, _) for _ in NUM_POINTS]))\n",
        "\n",
        "  times = np.array(times_list).mean(axis=0)\n",
        "\n",
        "  fig, axs = plt.subplots(1, 1, figsize=(5,4))\n",
        "  plt.plot(NUM_POINTS, times[:, 0], label='Forward')\n",
        "  plt.plot(NUM_POINTS, times[:, 1], label='Backward')\n",
        "  plt.xlabel('Number of data points')\n",
        "  plt.ylabel('Seconds')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "crR8MLLvtN8G"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Cost_of_computation_Interactive_Demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9zd-iI0LtN8G"
      },
      "source": [
        "## Coding Exercise 6: Implement minibatch sampling\n",
        "\n",
        "Complete the code in `sample_minibatch` so as to produce IID subsets of the training set of the desired size. (This is _not_ a trick question.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "AQlD_9MGtN8G"
      },
      "outputs": [],
      "source": [
        "def sample_minibatch(input_data, target_data, num_points=100):\n",
        "  \"\"\"\n",
        "  Sample a minibatch of size num_point from the provided input-target data\n",
        "\n",
        "  Args:\n",
        "    input_data: Tensor\n",
        "      Multi-dimensional tensor containing the input data\n",
        "    target_data: Tensor\n",
        "      1D tensor containing the class labels\n",
        "    num_points: Integer\n",
        "      Number of elements to be included in minibatch with default=100\n",
        "\n",
        "  Returns:\n",
        "    batch_inputs: Tensor\n",
        "      Minibatch inputs\n",
        "    batch_targets: Tensor\n",
        "      Minibatch targets\n",
        "  \"\"\"\n",
        "  #################################################\n",
        "  ## TODO for students: sample minibatch of data ##\n",
        "  raise NotImplementedError(\"Student exercise: implement gradient update\")\n",
        "  #################################################\n",
        "  # Sample a collection of IID indices from the existing data\n",
        "  batch_indices = ...\n",
        "  # Use batch_indices to extract entries from the input and target data tensors\n",
        "  batch_inputs = input_data[...]\n",
        "  batch_targets = target_data[...]\n",
        "\n",
        "  return batch_inputs, batch_targets\n",
        "\n",
        "\n",
        "\n",
        "## Uncomment to test your function\n",
        "# x_batch, y_batch = sample_minibatch(X, y, num_points=100)\n",
        "# print(f\"The input shape is {x_batch.shape} and the target shape is: {y_batch.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4kdy18C5tN8H"
      },
      "source": [
        "```\n",
        "The input shape is torch.Size([100, 28, 28]) and the target shape is: torch.Size([100])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "er1D30kltN8H"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_02847e9d.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "dXxpKoSetN8H"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Implement_mini_batch_sampling_Exercise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xvp8qN6dtN8H"
      },
      "source": [
        "## Interactive Demo 6.2: *Compare* different minibatch sizes\n",
        "\n",
        "What are the trade-offs induced by the choice of minibatch size? The interactive plot below shows the training evolution of a 2-hidden layer MLP with 100 hidden units in each hidden layer. Different plots correspond to a different choice of minibatch size. We have a fixed time budget for all the cases, reflected in the horizontal axes of these plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5siuwiC7tN8H"
      },
      "outputs": [],
      "source": [
        "# @markdown Execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact_manual\n",
        "def minibatch_experiment(batch_sizes='20, 250, 1000',\n",
        "                         lrs='5e-3, 5e-3, 5e-3',\n",
        "                         time_budget=widgets.Dropdown(options=[\"2.5\", \"5\", \"10\"],\n",
        "                                                      value=\"2.5\")):\n",
        "  \"\"\"\n",
        "  Demonstration of minibatch experiment\n",
        "\n",
        "  Args:\n",
        "    batch_sizes: String\n",
        "      Size of minibatches\n",
        "    lrs: String\n",
        "      Different learning rates\n",
        "    time_budget: widget dropdown instance\n",
        "      Different time budgets with default=2.5s\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  batch_sizes = [int(s) for s in batch_sizes.split(',')]\n",
        "  lrs = [float(s) for s in lrs.split(',')]\n",
        "\n",
        "  LOSS_HIST = {_:[] for _ in batch_sizes}\n",
        "\n",
        "  X, y = train_set.data, train_set.targets\n",
        "  base_model = MLP(in_dim=784, out_dim=10, hidden_dims=[100, 100])\n",
        "\n",
        "  for id, batch_size in enumerate(tqdm(batch_sizes)):\n",
        "    start_time = time.time()\n",
        "    # Create a new copy of the model for each batch size\n",
        "    model = copy.deepcopy(base_model)\n",
        "    params = list(model.parameters())\n",
        "    lr = lrs[id]\n",
        "    # Fixed budget per choice of batch size\n",
        "    while (time.time() - start_time) < float(time_budget):\n",
        "      data, labels = sample_minibatch(X, y, batch_size)\n",
        "      loss = loss_fn(model(data), labels)\n",
        "      gradient_update(loss, params, lr=lr)\n",
        "      LOSS_HIST[batch_size].append([time.time() - start_time,\n",
        "                                    loss.item()])\n",
        "\n",
        "  fig, axs = plt.subplots(1, len(batch_sizes), figsize=(10, 3))\n",
        "  for ax, batch_size in zip(axs, batch_sizes):\n",
        "    plot_data = np.array(LOSS_HIST[batch_size])\n",
        "    ax.plot(plot_data[:, 0], plot_data[:, 1], label=batch_size,\n",
        "            alpha=0.8)\n",
        "    ax.set_title('Batch size: ' + str(batch_size))\n",
        "    ax.set_xlabel('Seconds')\n",
        "    ax.set_ylabel('Loss')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "00rd2RI6tN8I"
      },
      "source": [
        "**Remarks:** SGD works! We have an algorithm that can be applied (with due precautions) to learn datasets of arbitrary size.\n",
        "\n",
        "However, **note the difference in the vertical scale** across the plots above. When using a larger minibatch, we can perform fewer parameter updates as the forward and backward passes are more expensive.\n",
        "\n",
        "This highlights the interplay between the minibatch size and the learning rate: when our minibatch is larger, we have a more confident estimator of the direction to move, and thus can afford a larger learning rate. On the other hand, extremely small minibatches are very fast computationally but are not representative of the data distribution and yield estimations of the gradient with high variance.\n",
        "\n",
        "We encourage you to tune the value of the learning rate for each of the minibatch sizes in the previous demo, to achieve a training loss steadily below 0.5 within 5 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "80Ihbb_WtN8I"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Compare_different_minibatch_sizes_Interactive_Demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZeGV19gMtN8I"
      },
      "source": [
        "---\n",
        "# Section 7: Adaptive methods\n",
        "\n",
        "*Time estimate: ~25 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rg_YOg4LtN8J"
      },
      "source": [
        "As of now, you should be aware that there are many knobs to turn when working on a machine learning problem. Some of these relate to the optimization algorithm, the choice of model, or the objective to minimize. Here are some prototypical examples:\n",
        "\n",
        "- Problem: loss function, regularization coefficients (Week 1, Day 5)\n",
        "- Model: architecture, activations function\n",
        "- Optimizer: learning rate, batch size, momentum coefficient\n",
        "\n",
        "We concentrate on the choices that are directly related to optimization. In particular, we will explore some _automatic_ methods for setting the learning rate in a way that fixes the poor-conditioning problem and is robust across different problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ypTkwS4DtN8J"
      },
      "outputs": [],
      "source": [
        "# @title Video 7: Adaptive Methods\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'Zr6r2kfmQUM'), ('Bilibili', 'BV1eq4y1W7JG')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "xfJnGg1jtN8J"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Adaptive_Methods_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "k-lwZXKStN8J"
      },
      "source": [
        "## Coding Exercise 7: Implement RMSprop\n",
        "\n",
        "In this exercise you will implement the update of the RMSprop optimizer:\n",
        "\n",
        "\\begin{align}\n",
        "v_{t} &= \\alpha v_{t-1} + (1 - \\alpha) \\nabla J(w_t)^2 \\\\ \\\\\n",
        "w_{t+1} &= w_t - \\eta \\frac{\\nabla J(w_t)}{\\sqrt{v_t + \\epsilon}}\n",
        "\\end{align}\n",
        "\n",
        "where the non-standard operations (the division of two vectors, squaring a vector, etc.) are to be interpreted as element-wise operations, i.e., the operation is applied to each (pair of) entry(ies) of the vector(s) considered as real number(s).\n",
        "\n",
        "Here, the $\\epsilon$ hyperparameter provides numerical stability to the algorithm by preventing the learning rate from becoming too big when $v_t$ is small. Typically, we set $\\epsilon$ to a small default value, like $10^{-8}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "RKBicw8btN8K"
      },
      "outputs": [],
      "source": [
        "def rmsprop_update(loss, params, grad_sq, lr=1e-3, alpha=0.8, epsilon=1e-8):\n",
        "  \"\"\"\n",
        "  Perform an RMSprop update on a collection of parameters\n",
        "\n",
        "  Args:\n",
        "    loss: Tensor\n",
        "      A scalar tensor containing the loss whose gradient will be computed\n",
        "    params: Iterable\n",
        "      Collection of parameters with respect to which we compute gradients\n",
        "    grad_sq: Iterable\n",
        "      Moving average of squared gradients\n",
        "    lr: Float\n",
        "      Scalar specifying the learning rate or step-size for the update\n",
        "    alpha: Float\n",
        "      Moving average parameter\n",
        "    epsilon: Float\n",
        "      quotient for numerical stability\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  # Clear up gradients as Pytorch automatically accumulates gradients from\n",
        "  # successive backward calls\n",
        "  zero_grad(params)\n",
        "  # Compute gradients on given objective\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for (par, gsq) in zip(params, grad_sq):\n",
        "      #################################################\n",
        "      ## TODO for students: update the value of the parameter ##\n",
        "      # Use gsq.data and par.grad\n",
        "      raise NotImplementedError(\"Student exercise: implement gradient update\")\n",
        "      #################################################\n",
        "      # Update estimate of gradient variance\n",
        "      gsq.data = ...\n",
        "      # Update parameters\n",
        "      par.data -=  ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "model3 = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
        "print('\\n The model3 parameters before the update are: \\n')\n",
        "print_params(model3)\n",
        "loss = loss_fn(model3(X), y)\n",
        "# Initialize the moving average of squared gradients\n",
        "grad_sq = [1e-6*i for i in list(model3.parameters())]\n",
        "\n",
        "\n",
        "\n",
        "## Uncomment below to test your function\n",
        "# rmsprop_update(loss, list(model3.parameters()), grad_sq=grad_sq, lr=1e-3)\n",
        "# print('\\n The model3 parameters after the update are: \\n')\n",
        "# print_params(model3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "x4H9QXyMtN8K"
      },
      "source": [
        "```\n",
        " The model3 parameters after the update are:\n",
        "\n",
        "main.0.weight tensor([[-0.0240,  0.0031,  0.0193,  ...,  0.0316,  0.0297, -0.0198],\n",
        "        [-0.0063, -0.0318, -0.0109,  ..., -0.0093,  0.0232, -0.0255],\n",
        "        [ 0.0218, -0.0253,  0.0320,  ...,  0.0102,  0.0248, -0.0203],\n",
        "        ...,\n",
        "        [-0.0027,  0.0136,  0.0089,  ...,  0.0123, -0.0324, -0.0166],\n",
        "        [ 0.0159,  0.0281,  0.0233,  ..., -0.0133, -0.0197,  0.0182],\n",
        "        [ 0.0186, -0.0376, -0.0205,  ..., -0.0293,  0.0077, -0.0019]])\n",
        "main.0.bias tensor([-0.0313, -0.0011,  0.0122, -0.0342,  0.0045,  0.0199,  0.0329,  0.0265,\n",
        "         0.0182, -0.0041])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "eQfZgG1utN8L"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_f7291fed.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "CBtbggFStN8L"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Implement_RMSProp_Exercise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "b_FJ56JgtN8L"
      },
      "source": [
        "## Interactive Demo 7: Compare optimizers\n",
        "\n",
        "Below, we compare your implementations of **SGD**, **Momentum**, and **RMSprop**. If you have successfully coded all the exercises so far: congrats!\n",
        "\n",
        "You are now *in the know* of some of the most commonly used and powerful optimization tools for deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "rG4dISbDtN8L"
      },
      "outputs": [],
      "source": [
        "# @markdown Execute this cell to enable the widget!\n",
        "X, y = train_set.data, train_set.targets\n",
        "\n",
        "@widgets.interact_manual\n",
        "def compare_optimizers(\n",
        "    batch_size=(25, 250, 5),\n",
        "    lr=widgets.FloatLogSlider(value=2e-3, min=-5, max=0),\n",
        "    max_steps=(50, 500, 5)):\n",
        "  \"\"\"\n",
        "  Demonstration to compare optimisers - stochastic gradient descent, momentum, RMSprop\n",
        "\n",
        "  Args:\n",
        "    batch_size: Tuple\n",
        "      Size of minibatches\n",
        "    lr: Float log slider instance\n",
        "      Scalar specifying the learning rate or step-size for the update\n",
        "    max_steps: Tuple\n",
        "      Max number of step sizes for incrementing\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  SGD_DICT = [gradient_update, 'SGD', 'black', '-', {'lr': lr}]\n",
        "  MOM_DICT = [momentum_update, 'Momentum', 'red', '--', {'lr': lr, 'beta': 0.9}]\n",
        "  RMS_DICT = [rmsprop_update, 'RMSprop', 'fuchsia', '-', {'lr': lr, 'alpha': 0.8}]\n",
        "\n",
        "  ALL_DICTS = [SGD_DICT, MOM_DICT, RMS_DICT]\n",
        "\n",
        "  base_model = MLP(in_dim=784, out_dim=10, hidden_dims=[100, 100])\n",
        "\n",
        "  LOSS_HIST = {}\n",
        "\n",
        "  for opt_dict in tqdm(ALL_DICTS):\n",
        "    update_fn, opt_name, color, lstyle, kwargs = opt_dict\n",
        "    LOSS_HIST[opt_name] = []\n",
        "\n",
        "    model = copy.deepcopy(base_model)\n",
        "    params = list(model.parameters())\n",
        "\n",
        "    if opt_name != 'SGD':\n",
        "      aux_tensors = [torch.zeros_like(_) for _ in params]\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      data, labels = sample_minibatch(X, y, batch_size)\n",
        "      loss = loss_fn(model(data), labels)\n",
        "      if opt_name == 'SGD':\n",
        "        update_fn(loss, params, **kwargs)\n",
        "      else:\n",
        "        update_fn(loss, params, aux_tensors, **kwargs)\n",
        "      LOSS_HIST[opt_name].append(loss.item())\n",
        "\n",
        "  fig, axs = plt.subplots(1, len(ALL_DICTS), figsize=(9, 3))\n",
        "  for ax, optim_dict in zip(axs, ALL_DICTS):\n",
        "    opt_name = optim_dict[1]\n",
        "    ax.plot(range(max_steps), LOSS_HIST[opt_name], alpha=0.8)\n",
        "    ax.set_title(opt_name)\n",
        "    ax.set_xlabel('Iteration')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_ylim(0, 2.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "pi-_0BiJtN8L"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Compare_optimizers_Interactive_Demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "DTNUsdJrtN8M"
      },
      "source": [
        "## Think 7.1!: Compare optimizers\n",
        "\n",
        "Tune the three methods above - **SGD**, **Momentum**, and **RMSProp** - to make each excel and discuss your findings. How do the methods compare in terms of robustness to small changes of the hyperparameters? How easy was it to find a good hyperparameter configuration?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "G7bVbvTytN8R"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_adc539df.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "cgb5HwFGtN8S"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Compare_optimizers_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OufBu534tN8S"
      },
      "source": [
        "**Remarks:** Note that RMSprop allows us to use a 'per-dimension' learning rate _without having to tune one learning rate for each dimension **ourselves**_. The method uses information collected about the variance of the gradients throughout training to **adapt** the step size for each of the parameters automatically. The savings in tuning efforts of RMSprop over SGD or 'plain' momentum are undisputed on this task.\n",
        "\n",
        "Moreover, adaptive optimization methods are currently a highly active research domain, with many related algorithms like Adam, AMSgrad, Adagrad being used in practical application and theoretically investigated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "zUNnsqv_tN8S"
      },
      "source": [
        "### Locality of Gradients\n",
        "\n",
        "As we've seen throughout this tutorial, poor conditioning can be a significant burden on convergence to an optimum while using gradient-based optimization. Of the methods we've seen to deal with this issue, notice how both momentum and adaptive learning rates incorporate past gradient values into their update schemes. Why do we use past values of our loss function's gradient while updating our current MLP weights?\n",
        "\n",
        "Recall from *W1D2* that the gradient of a function, $\\nabla f(w_t)$, is a **local** property and computes the direction of maximum change of $f(w_t)$ at the point $w_t$. However, when we train our MLP model we are hoping to find the **global** optimum for our training loss. By incorporating past values of our function's gradient into our optimization schemes, we use more information about the overall shape of our function than just a single gradient alone can provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aVHL7xR3tN8T"
      },
      "source": [
        "## Think! 7.2: Loss function and optimization\n",
        "\n",
        "Can you think of other ways we can incorporate more information about our loss function into our optimization schemes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "evLb8VMGtN8T"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_c7070297.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "NABZ2XgDtN8T"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Loss_function_and_optimization_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "S5XMTmGHtN8T"
      },
      "source": [
        "---\n",
        "# Section 8: Ethical concerns\n",
        "\n",
        "*Time estimate: ~15mins*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "o4o1d2eVtN8U"
      },
      "outputs": [],
      "source": [
        "# @title Video 8: Ethical concerns\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', '0EthSI0cknI'), ('Bilibili', 'BV1TU4y1G7Je')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "os57oifgtN8V"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Ethical_concerns_Video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "huUE_qLWtN8V"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "* Optimization is necessary to create Deep Learning models that are guaranteed to converge\n",
        "* Stochastic Gradient Descent and Momentum are two commonly used optimization techniques\n",
        "* RMSProp is a way of adaptive hyperparameter tuning which utilises a per-dimension learning rate\n",
        "* Poor choice of optimization objectives can lead to unforeseen, undesirable consequences\n",
        "\n",
        "If you have time left, you can read the Bonus material, where we put it all together and we compare our model with a benchmark model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "E9HLu-24tN8X"
      },
      "source": [
        "---\n",
        "# Daily survey\n",
        "\n",
        "Don't forget to complete your reflections and content check in the daily survey! Please be patient after logging in as there is\n",
        "a small delay before you will be redirected to the survey.\n",
        "\n",
        "<a href=\"https://portal.neuromatchacademy.org/api/redirect/to/2c5bbb85-d91a-4f5a-99fa-cefc287653d7\"><img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\" alt=\"button link to survey\" style=\"width:410px\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "K7GLUeTHtN8X"
      },
      "source": [
        "---\n",
        "# Bonus: Putting it all together\n",
        "\n",
        "*Time estimate: ~40 mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "tPOAkA4vtN8X"
      },
      "source": [
        "We have progressively built a sophisticated optimization algorithm, which is able to deal with a non-convex, poor-conditioned problem concerning tens of thousands of training examples. Now we present _you_ with a small challenge: beat us! :P\n",
        "\n",
        "Your mission is to train an MLP model that can compete with a benchmark model which we have pre-trained for you. In this section you will be able to use the full Pytorch power: loading the data, defining the model, sampling minibatches as well as Pytorch's **optimizer implementations**.\n",
        "\n",
        "There is a big engineering component behind the design of optimizers and their implementation can sometimes become tricky. So unless you are directly doing research in optimization, it's recommended to use an implementation provided by a widely reviewed open-source library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "2T10kk-wtN8Y"
      },
      "outputs": [],
      "source": [
        "# @title Video 9: Putting it all together\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'DP9c13vLiOM'), ('Bilibili', 'BV1MK4y1u7u2')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "IJ1aF6xhtN8Y"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Putting_it_all_together_Bonus_Video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "fbKoEWpVtN8Y"
      },
      "outputs": [],
      "source": [
        "# @title Download parameters of the benchmark model\n",
        "import requests\n",
        "\n",
        "fname = 'benchmark_model.pt'\n",
        "url = \"https://osf.io/sj4e8/download\"\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "with open(fname, 'wb') as fh:\n",
        "  fh.write(r.content)\n",
        "\n",
        "# Load the benchmark model's parameters\n",
        "DEVICE = set_device()\n",
        "if DEVICE == \"cuda\":\n",
        "  benchmark_state_dict = torch.load(fname)\n",
        "else:\n",
        "  benchmark_state_dict = torch.load(fname, map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "xeBhgZcLtN8Y"
      },
      "outputs": [],
      "source": [
        "# Create MLP object and update weights with those of saved model\n",
        "benchmark_model = MLP(in_dim=784, out_dim=10,\n",
        "                      hidden_dims=[200, 100, 50]).to(DEVICE)\n",
        "benchmark_model.load_state_dict(benchmark_state_dict)\n",
        "\n",
        "\n",
        "# Define helper function to evaluate models\n",
        "def eval_model(model, data_loader, num_batches=np.inf, device='cpu'):\n",
        "  \"\"\"\n",
        "  To evaluate a given model\n",
        "\n",
        "  Args:\n",
        "    model: nn.Module derived class\n",
        "      The model which is to be evaluated\n",
        "    data_loader: Iterable\n",
        "      A configured dataloading utility\n",
        "    num_batches: Integer\n",
        "      Size of minibatches\n",
        "    device: String\n",
        "      Sets the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Returns:\n",
        "    mean of log loss and mean of log accuracy\n",
        "  \"\"\"\n",
        "\n",
        "  loss_log, acc_log = [], []\n",
        "  model.to(device=device)\n",
        "\n",
        "  # We are just evaluating the model, no need to compute gradients\n",
        "  with torch.no_grad():\n",
        "    for batch_id, batch in enumerate(data_loader):\n",
        "      # If we only evaluate a number of batches, stop after we reach that number\n",
        "      if batch_id > num_batches:\n",
        "        break\n",
        "      # Extract minibatch data\n",
        "      data, labels = batch[0].to(device), batch[1].to(device)\n",
        "      # Evaluate model and loss on minibatch\n",
        "      preds = model(data)\n",
        "      loss_log.append(loss_fn(preds, labels).item())\n",
        "      acc_log.append(torch.mean(1. * (preds.argmax(dim=1) == labels)).item())\n",
        "\n",
        "  return np.mean(loss_log), np.mean(acc_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aoq0N1tItN8Z"
      },
      "source": [
        "We define an optimizer in the following steps:\n",
        "\n",
        "1. Load  the corresponding class that implements the parameter updates and other internal management activities, including:\n",
        "    - create auxiliary variables,\n",
        "    - update moving averages,\n",
        "    - adjust the learning rate.\n",
        "2. Pass the parameters of the Pytorch model that the optimizer has control over. Note that different optimizers can potentially control different parameter groups.\n",
        "3. Specify hyperparameters, including learning rate, momentum, moving average factors, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TneYD-7PtN8k"
      },
      "source": [
        "## Exercise Bonus: Train your own model\n",
        "\n",
        "Now, train the model with your preferred optimizer and find a good combination of hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Oy9wg8TwtN8k"
      },
      "outputs": [],
      "source": [
        "#################################################\n",
        "## TODO for students: adjust training settings ##\n",
        "\n",
        "# The three parameters below are in your full control\n",
        "MAX_EPOCHS = 2  # select number of epochs to train\n",
        "LR = 1e-5  # choose the step size\n",
        "BATCH_SIZE = 64  # number of examples per minibatch\n",
        "\n",
        "# Define the model and associated optimizer -- you may change its architecture!\n",
        "my_model = MLP(in_dim=784, out_dim=10, hidden_dims=[200, 100, 50]).to(DEVICE)\n",
        "\n",
        "# You can take your pick from many different optimizers\n",
        "# Check the optimizer documentation and hyperparameter meaning before using!\n",
        "# More details on Pytorch optimizers: https://pytorch.org/docs/stable/optim.html\n",
        "# optimizer = torch.optim.SGD(my_model.parameters(), lr=LR, momentum=0.9)\n",
        "# optimizer = torch.optim.RMSprop(my_model.parameters(), lr=LR, alpha=0.99)\n",
        "# optimizer = torch.optim.Adagrad(my_model.parameters(), lr=LR)\n",
        "optimizer = torch.optim.Adam(my_model.parameters(), lr=LR)\n",
        "#################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "-3lBtWVatN8l"
      },
      "outputs": [],
      "source": [
        "set_seed(seed=SEED)\n",
        "# Print training stats every LOG_FREQ minibatches\n",
        "LOG_FREQ = 200\n",
        "# Frequency for evaluating the validation metrics\n",
        "VAL_FREQ = 200\n",
        "# Load data using a Pytorch Dataset\n",
        "train_set_orig, test_set_orig = load_mnist_data(change_tensors=False)\n",
        "\n",
        "# We separate 10,000 training samples to create a validation set\n",
        "train_set_orig, val_set_orig = torch.utils.data.random_split(train_set_orig, [50000, 10000])\n",
        "\n",
        "# Create the corresponding DataLoaders for training and test\n",
        "g_seed = torch.Generator()\n",
        "g_seed.manual_seed(SEED)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set_orig,\n",
        "                                           shuffle=True,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           num_workers=2,\n",
        "                                           worker_init_fn=seed_worker,\n",
        "                                           generator=g_seed)\n",
        "val_loader = torch.utils.data.DataLoader(val_set_orig,\n",
        "                                         shuffle=True,\n",
        "                                         batch_size=256,\n",
        "                                         num_workers=2,\n",
        "                                         worker_init_fn=seed_worker,\n",
        "                                         generator=g_seed)\n",
        "test_loader = torch.utils.data.DataLoader(test_set_orig,\n",
        "                                          batch_size=256,\n",
        "                                          num_workers=2,\n",
        "                                          worker_init_fn=seed_worker,\n",
        "                                          generator=g_seed)\n",
        "\n",
        "# Run training\n",
        "metrics = {'train_loss':[],\n",
        "           'train_acc':[],\n",
        "           'val_loss':[],\n",
        "           'val_acc':[],\n",
        "           'val_idx':[]}\n",
        "\n",
        "step_idx = 0\n",
        "for epoch in tqdm(range(MAX_EPOCHS)):\n",
        "\n",
        "  running_loss, running_acc = 0., 0.\n",
        "\n",
        "  for batch_id, batch in enumerate(train_loader):\n",
        "    step_idx += 1\n",
        "    # Extract minibatch data and labels\n",
        "    data, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
        "    # Just like before, refresh gradient accumulators.\n",
        "    # Note that this is now a method of the optimizer.\n",
        "    optimizer.zero_grad()\n",
        "    # Evaluate model and loss on minibatch\n",
        "    preds = my_model(data)\n",
        "    loss = loss_fn(preds, labels)\n",
        "    acc = torch.mean(1.0 * (preds.argmax(dim=1) == labels))\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "    # Update parameters\n",
        "    # Note how all the magic in the update of the parameters is encapsulated by\n",
        "    # the optimizer class.\n",
        "    optimizer.step()\n",
        "    # Log metrics for plotting\n",
        "    metrics['train_loss'].append(loss.cpu().item())\n",
        "    metrics['train_acc'].append(acc.cpu().item())\n",
        "\n",
        "    if batch_id % VAL_FREQ == (VAL_FREQ - 1):\n",
        "      # Get an estimate of the validation accuracy with 100 batches\n",
        "      val_loss, val_acc = eval_model(my_model, val_loader,\n",
        "                                     num_batches=100,\n",
        "                                     device=DEVICE)\n",
        "      metrics['val_idx'].append(step_idx)\n",
        "      metrics['val_loss'].append(val_loss)\n",
        "      metrics['val_acc'].append(val_acc)\n",
        "\n",
        "      print(f\"[VALID] Epoch {epoch + 1} - Batch {batch_id + 1} - \"\n",
        "            f\"Loss: {val_loss:.3f} - Acc: {100*val_acc:.3f}%\")\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.cpu().item()\n",
        "    running_acc += acc.cpu().item()\n",
        "    # Print every LOG_FREQ minibatches\n",
        "    if batch_id % LOG_FREQ == (LOG_FREQ-1):\n",
        "      print(f\"[TRAIN] Epoch {epoch + 1} - Batch {batch_id + 1} - \"\n",
        "            f\"Loss: {running_loss / LOG_FREQ:.3f} - \"\n",
        "            f\"Acc: {100 * running_acc / LOG_FREQ:.3f}%\")\n",
        "\n",
        "      running_loss, running_acc = 0., 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "TRnxKpZZtN8l"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "ax[0].plot(range(len(metrics['train_loss'])), metrics['train_loss'],\n",
        "           alpha=0.8, label='Train')\n",
        "ax[0].plot(metrics['val_idx'], metrics['val_loss'], label='Valid')\n",
        "ax[0].set_xlabel('Iteration')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(range(len(metrics['train_acc'])), metrics['train_acc'],\n",
        "           alpha=0.8, label='Train')\n",
        "ax[1].plot(metrics['val_idx'], metrics['val_acc'], label='Valid')\n",
        "ax[1].set_xlabel('Iteration')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "hnVBos7StN8l"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Train_your_own_model_Bonus_Exercise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "oJzNhUI3tN8m"
      },
      "source": [
        "## Think! Bonus: Metrics\n",
        "\n",
        "Which metric did you optimize when searching for the right configuration? The training set loss? Accuracy? Validation/test set metrics? Why? Discuss!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bmCRMH0mtN8m"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W1D5_Optimization/solutions/W1D5_Tutorial1_Solution_093a66ad.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "UXSRyX6TtN8m"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_Metrics_Bonus_Discussion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "QbGudh7qtN8n"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "We _finally_ can evaluate and compare the performance of the models on previously unseen examples.\n",
        "\n",
        "Which model would you keep? (\\*drum roll*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "KaPDuxW0tN8n"
      },
      "outputs": [],
      "source": [
        "print('Your model...')\n",
        "train_loss, train_accuracy = eval_model(my_model, train_loader, device=DEVICE)\n",
        "test_loss, test_accuracy = eval_model(my_model, test_loader, device=DEVICE)\n",
        "print(f'Train Loss {train_loss:.3f} / Test Loss {test_loss:.3f}')\n",
        "print(f'Train Accuracy {100*train_accuracy:.3f}% / Test Accuracy {100*test_accuracy:.3f}%')\n",
        "\n",
        "print('\\nBenchmark model')\n",
        "train_loss, train_accuracy = eval_model(benchmark_model, train_loader, device=DEVICE)\n",
        "test_loss, test_accuracy = eval_model(benchmark_model, test_loader, device=DEVICE)\n",
        "print(f'Train Loss {train_loss:.3f} / Test Loss {test_loss:.3f}')\n",
        "print(f'Train Accuracy {100*train_accuracy:.3f}% / Test Accuracy {100*test_accuracy:.3f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "W1D5_Tutorial1",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "toc-autonumbering": true,
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00df399a49ed4e04bf4ef78c8a8abfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TabModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TabModel",
            "_titles": {
              "0": "Youtube",
              "1": "Bilibili"
            },
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TabView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d95c9825b2cf40bd8bfcccfa42813398",
              "IPY_MODEL_149027270be24c2a8c0c2d3f04e9f202"
            ],
            "layout": "IPY_MODEL_24f1b984b05748ecbe4f259112598faf",
            "selected_index": 0
          }
        },
        "d95c9825b2cf40bd8bfcccfa42813398": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c3ad5d57a5384513b8f0c52795635ce8",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://youtube.com/watch?v=zm9oekdkJbQ\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.lib.display.YouTubeVideo at 0x7adae42a3510>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/zm9oekdkJbQ?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        ",
                  "image/jpeg": "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\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "149027270be24c2a8c0c2d3f04e9f202": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b91a6732a62b44d8ad79ca05d97be2ea",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://www.bilibili.com/video/BV1VB4y1K7Vr\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<__main__.PlayVideo at 0x7adae43b5550>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://player.bilibili.com/player.html?bvid=BV1VB4y1K7Vr&page=1?fs=1&autoplay=False\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
                },
                "metadata": {}
              }
            ]
          }
        },
        "24f1b984b05748ecbe4f259112598faf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ad5d57a5384513b8f0c52795635ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91a6732a62b44d8ad79ca05d97be2ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}